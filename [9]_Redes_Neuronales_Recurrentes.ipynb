{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jurados/NotesPytorch/blob/main/%5B9%5D_Redes_Neuronales_Recurrentes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pExV9tXpDb2_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capítulo 13: Redes Neuronales Recurrentes"
      ],
      "metadata": {
        "id": "bbPs43Ig6sBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las Redes Neuronales Recurrentes (RNN) son una clase de redes preparadas para analizar datos de series temporales, y permiten tratar la dimensión de \"tiempo\".\n",
        "\n",
        "Cada neurona recurrente tienen dos conjuntos de parámetros, uno que lo aplica a la entrada de datos que reibe de la capa anterior y otro que lo aplica a la entrada de datos correspondientes al vector de salida del instante anterior.\n",
        "\n",
        "$$y_{t} = f\\left(W\\cdot x_{i} + U \\cdot y_{i} + b\\right)$$\n",
        "\n",
        "Donde $X = \\{x_{1}, \\ldots , x_{T}\\}$ representan la secuencia de entrada proveniente de la capa anterior,$W$ la matriz de pesos y $b$ el sesgos visto ya en las capas anteriores. $U$ es la matrzi de pesos que opera sobre el estado de la red en el instante de tiempo anterior $y_{t-1}$.\n",
        "\n",
        "*Exploding Gradients:* Se da cuando el algoritmo asigna una importancia exageradamente alta a los pesos sin mucha razón, y esto genera un problema en el entrenamiento.\n",
        "*Vanishing Gradients:* Se da cuando los valores de un gradiente son demasiado pequeños y el modelo deja de aprender o requiere demasiado tiempo debido a ello."
      ],
      "metadata": {
        "id": "YfD6l3R76xI_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}